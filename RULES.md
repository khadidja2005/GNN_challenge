# ğŸ“œ ENZYMES-Hard Challenge Rules

## ğŸ¯ Objective

Build a Graph Neural Network that achieves the highest Macro F1-score on the hidden test set while adhering to all constraints.

---

## ğŸ“‹ Complete Rules

### 1. Model Constraints

#### 1.1 Parameter Limit
- **Maximum 100,000 trainable parameters**
- This includes all layers: embeddings, convolutions, normalization, linear layers
- Use `sum(p.numel() for p in model.parameters() if p.requires_grad)` to count

#### 1.2 Training Time
- **Maximum 5 minutes on CPU**
- Reference hardware: Intel Core i5-10400 or equivalent
- Training includes all preprocessing done at training time
- Data preparation time (one-time) is not counted

#### 1.3 Architecture
- Any GNN architecture is allowed (GCN, GAT, GIN, GraphSAGE, etc.)
- Custom layers and pooling are allowed
- Must be implemented in PyTorch/PyTorch Geometric

### 2. Data Rules

#### 2.1 Training Data
- **Only use the provided 240 training graphs**
- No external protein databases or datasets
- No pre-computed features from external sources

#### 2.2 Validation Data
- Use the 180 validation graphs for hyperparameter tuning
- Validation labels are provided

#### 2.3 Test Data
- 180 test graphs with hidden labels
- Contains edge dropout (10% edges removed)
- Contains missing features (same as validation)

#### 2.4 Data Augmentation
- âœ… **Allowed**: Feature noise, node dropout, edge perturbation on training data
- âœ… **Allowed**: Graph-level augmentations (subgraph sampling, etc.)
- âŒ **Not Allowed**: Using external data to augment
- âŒ **Not Allowed**: Using validation/test data for training

### 3. Pre-training Rules

#### 3.1 Pre-trained Models
- âŒ **Not Allowed**: Pre-trained GNN weights from other datasets
- âŒ **Not Allowed**: Transfer learning from external models
- âœ… **Allowed**: Standard weight initialization (Xavier, Kaiming, etc.)

#### 3.2 Self-supervised Pre-training
- âœ… **Allowed**: Self-supervised pre-training on the training set only
- Examples: Contrastive learning, graph reconstruction, etc.

### 4. Submission Requirements

#### 4.1 Prediction File
- CSV format with columns: `graph_id`, `prediction`
- 180 rows (one per test graph)
- Predictions must be integers 1-6

#### 4.2 Code Submission
- Complete, reproducible code required
- Include all source files and a `train.py` script
- Must include requirements/dependencies list
- README with instructions to reproduce
- **Docker support recommended**: Include Dockerfile for reproducibility

#### 4.3 Reproducibility
- Set random seed (we recommend 42)
- Results should be reproducible within Â±2% F1
- Include trained model weights

### 5. Evaluation

#### 5.1 Metrics
- **Primary**: Macro F1-score (average F1 across all classes)
- **Secondary**: Accuracy (for reference)

#### 5.2 Ranking
- Ranked by Macro F1-score (higher is better)
- Ties broken by Accuracy
- Further ties broken by parameter count (fewer is better)

#### 5.3 Validation
- Submissions will be checked for rule compliance
- Parameter count will be verified
- Training time will be tested on reference hardware

### 6. Disqualification Criteria

You will be disqualified if:
- âŒ Model exceeds 100,000 parameters
- âŒ Training takes more than 5 minutes
- âŒ Using external data or pre-trained weights
- âŒ Manually labeling test data
- âŒ Submitting predictions not generated by your model
- âŒ Using validation/test data for training
- âŒ Code is not reproducible

---

## ğŸ’¡ Frequently Asked Questions

### Q: Can I use ensemble methods?
**A**: Yes, but the total parameter count of all models combined must be â‰¤100K.

### Q: Can I use graph-level features I compute myself?
**A**: Yes, as long as they're computed only from the provided graph structure and features.

### Q: Can I use different architectures for different graphs?
**A**: Yes, but all architectures count toward the parameter limit.

### Q: What if my code runs faster on my machine?
**A**: We test on reference hardware. Optimize for the 5-minute limit with margin.

### Q: Can I precompute node embeddings?
**A**: If the precomputation is part of training, it counts toward training time.

### Q: How do I handle missing (NaN) features?
**A**: This is part of the challenge! Common approaches include imputation, masking, or learned handling.

### Q: Can I use attention mechanisms?
**A**: Yes, attention (GAT, Transformer, etc.) is allowed.

### Q: What PyTorch Geometric version should I use?
**A**: Use the version in `requirements.txt` for consistency.

---

## ğŸ“Š Scoring Example

```python
from sklearn.metrics import f1_score, accuracy_score

# Your predictions
predictions = [...]  # List of predicted classes (1-6)
ground_truth = [...]  # Hidden test labels

# Macro F1 (primary metric)
macro_f1 = f1_score(ground_truth, predictions, average='macro')

# Accuracy (secondary metric)
accuracy = accuracy_score(ground_truth, predictions)
```

### Running Evaluation with Docker

```bash
# Evaluate your predictions
docker-compose run gnn python scripts/evaluate.py --predictions submissions/your_predictions.csv
```

---

## ğŸ† Prizes

This is an educational challenge focused on learning. Winners receive:
- ğŸ¥‡ Recognition in the repository
- ğŸ“ Featured solution write-up
- ğŸŒŸ GitHub profile showcase

---

## ğŸ“§ Rule Clarifications

If you have questions about the rules, please open an issue with the tag `[RULES]`.

---

*Last updated: November 2025*
