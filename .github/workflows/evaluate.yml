name: Evaluate Submission

on:
  pull_request:
    paths:
      - 'submissions/**'
  workflow_dispatch:
    inputs:
      submission_path:
        description: 'Path to submission directory'
        required: true
        default: 'submissions/example'

jobs:
  validate:
    runs-on: ubuntu-latest
    name: Validate Submission
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install numpy pandas scikit-learn torch
          pip install torch-geometric -f https://data.pyg.org/whl/torch-2.0.0+cpu.html
      
      - name: Prepare challenge data
        run: |
          python scripts/prepare_data.py --seed 42
      
      - name: Find predictions file
        id: find_predictions
        run: |
          # Find the predictions.csv file in the submission
          if [ -n "${{ github.event.inputs.submission_path }}" ]; then
            SUBMISSION_PATH="${{ github.event.inputs.submission_path }}"
          else
            # For PRs, find changed submission directories
            SUBMISSION_PATH=$(git diff --name-only origin/main | grep "^submissions/" | head -1 | xargs dirname)
          fi
          
          PREDICTIONS_FILE="${SUBMISSION_PATH}/predictions.csv"
          
          if [ -f "$PREDICTIONS_FILE" ]; then
            echo "predictions_file=$PREDICTIONS_FILE" >> $GITHUB_OUTPUT
            echo "‚úÖ Found predictions file: $PREDICTIONS_FILE"
          else
            echo "‚ùå No predictions.csv found in submission"
            exit 1
          fi
      
      - name: Validate submission format
        run: |
          python -c "
          import pandas as pd
          import sys
          
          df = pd.read_csv('${{ steps.find_predictions.outputs.predictions_file }}')
          
          errors = []
          
          # Check columns
          if 'graph_id' not in df.columns:
              errors.append('Missing graph_id column')
          if 'prediction' not in df.columns:
              errors.append('Missing prediction column')
          
          # Check row count
          if len(df) != 180:
              errors.append(f'Expected 180 predictions, got {len(df)}')
          
          # Check prediction range
          if 'prediction' in df.columns:
              invalid = df[(df['prediction'] < 1) | (df['prediction'] > 6)]
              if len(invalid) > 0:
                  errors.append(f'Invalid predictions: {invalid[\"prediction\"].unique().tolist()}')
          
          # Check duplicates
          if df['graph_id'].duplicated().any():
              errors.append('Duplicate graph_ids found')
          
          if errors:
              print('‚ùå Validation errors:')
              for e in errors:
                  print(f'   - {e}')
              sys.exit(1)
          else:
              print('‚úÖ Submission format valid')
          "
      
      - name: Evaluate on validation set
        run: |
          python scripts/evaluate.py \
            --predictions ${{ steps.find_predictions.outputs.predictions_file }} \
            --ground_truth val \
            --detailed
      
      - name: Post results as comment
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            
            // Read evaluation results if available
            let results = '‚è≥ Evaluation pending...';
            
            try {
              const resultsPath = '${{ steps.find_predictions.outputs.predictions_file }}'.replace('predictions.csv', 'evaluation_results.json');
              if (fs.existsSync(resultsPath)) {
                const data = JSON.parse(fs.readFileSync(resultsPath, 'utf8'));
                results = `
            ## üìä Evaluation Results
            
            | Metric | Score |
            |--------|-------|
            | **Macro F1** | ${data.macro_f1.toFixed(4)} |
            | Accuracy | ${data.accuracy.toFixed(4)} |
            | Weighted F1 | ${data.weighted_f1.toFixed(4)} |
            
            *Evaluated on validation set*
            `;
              }
            } catch (e) {
              results = '‚ùå Could not read evaluation results';
            }
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: results
            });

  check-constraints:
    runs-on: ubuntu-latest
    name: Check Model Constraints
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install numpy pandas torch
          pip install torch-geometric -f https://data.pyg.org/whl/torch-2.0.0+cpu.html
      
      - name: Check for model code
        run: |
          echo "üîç Checking for model code in submission..."
          # This is a placeholder - in a real scenario, you'd run the model
          # and check parameter count and training time
          echo "‚ö†Ô∏è Manual review required for:"
          echo "   - Parameter count (‚â§100K)"
          echo "   - Training time (‚â§5 minutes)"
          echo "   - No external data usage"
