name: Evaluate Submission

on:
  pull_request:
    paths:
      - 'submissions/**'
      - 'scripts/**'
  workflow_dispatch:
    inputs:
      submission_path:
        description: 'Path to submission directory (e.g., submissions/your_team)'
        required: true
        default: 'submissions/example'
      ground_truth:
        description: 'Split to evaluate against'
        required: false
        default: 'val'

jobs:
  evaluate:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      pull-requests: write
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python 3.10
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install numpy pandas scikit-learn torch
          pip install torch-geometric -f https://data.pyg.org/whl/torch-2.0.0+cpu.html

      - name: Prepare challenge data
        run: python scripts/prepare_data.py --seed 42

      - name: Locate predictions file
        id: find_predictions
        shell: bash
        run: |
          SUBMISSION_PATH="${{ github.event.inputs.submission_path }}"
          if [ -z "$SUBMISSION_PATH" ]; then
            SUBMISSION_PATH=$(git ls-files 'submissions/*/predictions.csv' | head -1 | xargs dirname)
          fi
          if [ -z "$SUBMISSION_PATH" ]; then
            echo "No submission path detected." >&2
            exit 1
          fi
          PRED_FILE="$SUBMISSION_PATH/predictions.csv"
          if [ ! -f "$PRED_FILE" ]; then
            echo "predictions.csv not found at $PRED_FILE" >&2
            exit 1
          fi
          echo "predictions_file=$PRED_FILE" >> "$GITHUB_OUTPUT"
          echo "submission_path=$SUBMISSION_PATH" >> "$GITHUB_OUTPUT"

      - name: Validate submission format
        run: |
          pred_file="${{ steps.find_predictions.outputs.predictions_file }}"
          python scripts/validate_submission.py --predictions "$pred_file"

      - name: Evaluate on validation split
        run: |
          GT="${{ github.event.inputs.ground_truth }}"
          if [ -z "$GT" ]; then GT="val"; fi
          python scripts/evaluate.py \
            --predictions "${{ steps.find_predictions.outputs.predictions_file }}" \
            --ground_truth "$GT" \
            --detailed

      - name: Collect metrics
        id: metrics
        run: |
          RESULTS_PATH="$(dirname "${{ steps.find_predictions.outputs.predictions_file }}")/evaluation_results.json"
          echo "results_path=$RESULTS_PATH" >> "$GITHUB_OUTPUT"
          echo "macro_f1=$(jq -r '.macro_f1' "$RESULTS_PATH")" >> "$GITHUB_OUTPUT"
          echo "accuracy=$(jq -r '.accuracy' "$RESULTS_PATH")" >> "$GITHUB_OUTPUT"
          echo "weighted_f1=$(jq -r '.weighted_f1' "$RESULTS_PATH")" >> "$GITHUB_OUTPUT"

      - name: Upload evaluation artifact
        uses: actions/upload-artifact@v4
        with:
          name: evaluation-results
          path: |
            ${{ steps.metrics.outputs.results_path }}
            ${{ steps.find_predictions.outputs.predictions_file }}

      - name: Comment results on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const macro = Number('${{ steps.metrics.outputs.macro_f1 }}').toFixed(4);
            const acc = Number('${{ steps.metrics.outputs.accuracy }}').toFixed(4);
            const weighted = Number('${{ steps.metrics.outputs.weighted_f1 }}').toFixed(4);
            const submissionPath = '${{ steps.find_predictions.outputs.submission_path }}';
            const body = [
              '## ðŸ“Š Validation Evaluation',
              '',
              '| Metric | Score |',
              '|--------|-------|',
              `| **Macro F1** | ${macro} |`,
              `| Accuracy | ${acc} |`,
              `| Weighted F1 | ${weighted} |`,
              '',
              `Submission path: ${submissionPath}`,
              '',
              '_Results generated by CI on the validation split. Use the "Update Leaderboard" workflow after merging to publish to the site._',
            ].join('\n');

            await github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body,
            });
