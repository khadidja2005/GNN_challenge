name: Evaluate Submission

on:
  pull_request:
    paths:
      - 'submissions/**'
      - 'scripts/**'
  workflow_dispatch:
    inputs:
      submission_path:
        description: 'Path to submission directory (e.g., submissions/your_team)'
        required: true
        default: 'submissions/example'
      ground_truth:
        description: 'Split to evaluate against'
        required: false
        default: 'val'

jobs:
  evaluate:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      pull-requests: write
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python 3.10
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install numpy pandas scikit-learn torch
          pip install torch-geometric -f https://data.pyg.org/whl/torch-2.0.0+cpu.html

      - name: Prepare challenge data
        run: python scripts/prepare_data.py --seed 42

      - name: Locate predictions file
        id: find_predictions
        shell: bash
        run: |
          if [ -n "${{ github.event.inputs.submission_path }}" ]; then
            SUBMISSION_PATH="${{ github.event.inputs.submission_path }}"
          else
            SUBMISSION_PATH=$(git ls-files 'submissions/*/predictions.csv' | head -1 | xargs dirname)
          fi

          if [ -z "$SUBMISSION_PATH" ]; then
            echo "No submission path detected." >&2
            exit 1
          fi

          PRED_FILE="$SUBMISSION_PATH/predictions.csv"

          if [ ! -f "$PRED_FILE" ]; then
            echo "predictions.csv not found at $PRED_FILE" >&2
            exit 1
          fi

          echo "predictions_file=$PRED_FILE" >> "$GITHUB_OUTPUT"
          echo "submission_path=$SUBMISSION_PATH" >> "$GITHUB_OUTPUT"

      - name: Validate submission format
        run: |
          python - <<'PY'
import pandas as pd
import sys

pred_file = "${{ steps.find_predictions.outputs.predictions_file }}"
df = pd.read_csv(pred_file)

errors = []
if 'graph_id' not in df.columns:
    errors.append("Missing 'graph_id' column")
if 'prediction' not in df.columns:
    errors.append("Missing 'prediction' column")
if len(df) != 180:
    errors.append(f"Expected 180 predictions, got {len(df)}")
if 'prediction' in df.columns:
    invalid = df[(df['prediction'] < 1) | (df['prediction'] > 6)]
    if len(invalid) > 0:
        errors.append(f"Invalid predictions (not in 1-6): {invalid['prediction'].unique().tolist()}")
if df['graph_id'].duplicated().any():
    errors.append('Duplicate graph_id entries found')

if errors:
    print('‚ùå Validation errors:')
    for e in errors:
        print('  -', e)
    sys.exit(1)
print('‚úÖ Submission format valid')
PY

      - name: Evaluate on validation split
        run: |
          python scripts/evaluate.py \
            --predictions "${{ steps.find_predictions.outputs.predictions_file }}" \
            --ground_truth "${{ github.event.inputs.ground_truth || 'val' }}" \
            --detailed

      - name: Collect metrics
        id: metrics
        run: |
          python - <<'PY'
import json
from pathlib import Path

results_path = Path("${{ steps.find_predictions.outputs.predictions_file }}").parent / "evaluation_results.json"
data = json.loads(results_path.read_text())

with open(Path("${GITHUB_OUTPUT}"), 'a', encoding='utf-8') as fh:
  fh.write(f"macro_f1={data['macro_f1']}\n")
  fh.write(f"accuracy={data['accuracy']}\n")
  fh.write(f"weighted_f1={data['weighted_f1']}\n")
  fh.write(f"results_path={results_path}\n")
PY

      - name: Upload evaluation artifact
        uses: actions/upload-artifact@v4
        with:
          name: evaluation-results
          path: |
            ${{ steps.metrics.outputs.results_path }}
            ${{ steps.find_predictions.outputs.predictions_file }}

      - name: Comment results on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const macro = Number('${{ steps.metrics.outputs.macro_f1 }}').toFixed(4);
            const acc = Number('${{ steps.metrics.outputs.accuracy }}').toFixed(4);
            const weighted = Number('${{ steps.metrics.outputs.weighted_f1 }}').toFixed(4);
            const submissionPath = '${{ steps.find_predictions.outputs.submission_path }}';
            const body = `## üìä Validation Evaluation

| Metric | Score |
|--------|-------|
| **Macro F1** | ${macro} |
| Accuracy | ${acc} |
| Weighted F1 | ${weighted} |

Submission path: ${submissionPath}

_Results generated by CI on the validation split. Use the "Update Leaderboard" workflow after merging to publish to the site._`;

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body
            });
