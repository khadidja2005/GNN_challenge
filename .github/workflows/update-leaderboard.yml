name: Update Leaderboard

on:
  workflow_dispatch:
    inputs:
      submission_path:
        description: 'Path to submission directory (e.g., submissions/team-name)'
        required: true
      team_name:
        description: 'Display name for the leaderboard entry'
        required: true
      params:
        description: 'Model parameter count (e.g., 72K)'
        required: false
        default: 'N/A'
      train_time:
        description: 'Training time (e.g., ~3m 10s)'
        required: false
        default: 'N/A'
      is_baseline:
        description: 'Mark as baseline entry'
        required: false
        default: 'false'
      submitted_at:
        description: 'Optional submission label (date/PR/commit)'
        required: false
        default: ''

permissions:
  contents: write
  pull-requests: write

jobs:
  update:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python 3.10
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install numpy pandas scikit-learn torch
          pip install torch-geometric -f https://data.pyg.org/whl/torch-2.0.0+cpu.html

      - name: Prepare challenge data
        run: python scripts/prepare_data.py --seed 42

      - name: Evaluate submission on validation split
        run: |
          python scripts/evaluate.py \
            --predictions "${{ inputs.submission_path }}/predictions.csv" \
            --ground_truth val \
            --detailed

      - name: Extract metrics
        id: metrics
        run: |
          python - <<'PY'
import json
from pathlib import Path

results_path = Path("${{ inputs.submission_path }}") / "evaluation_results.json"
if not results_path.exists():
    raise SystemExit(f"results file not found: {results_path}")

data = json.loads(results_path.read_text())
with open(Path("${GITHUB_OUTPUT}"), 'a', encoding='utf-8') as fh:
  fh.write(f"macro_f1={data['macro_f1']}\n")
  fh.write(f"accuracy={data['accuracy']}\n")
  fh.write(f"weighted_f1={data['weighted_f1']}\n")
  fh.write(f"results_path={results_path}\n")
PY

      - name: Update leaderboard data
        id: update_board
        run: |
          python - <<'PY'
import json
from pathlib import Path
from datetime import datetime

leaderboard_path = Path('web/src/app/leaderboard/data.json')
if not leaderboard_path.exists():
    raise SystemExit('Leaderboard data file missing: ' + str(leaderboard_path))

data = json.loads(leaderboard_path.read_text())

new_entry = {
    "rank": 0,  # temp; will be reassigned
    "name": "${{ inputs.team_name }}",
    "macroF1": float("${{ steps.metrics.outputs.macro_f1 }}"),
    "accuracy": float("${{ steps.metrics.outputs.accuracy }}"),
    "params": "${{ inputs.params }}",
    "trainTime": "${{ inputs.train_time }}",
    "submittedAt": "${{ inputs.submitted_at }}" or datetime.utcnow().isoformat(),
}
if "${{ inputs.is_baseline }}".lower() == 'true':
    new_entry["isBaseline"] = True

# Replace existing entry with the same name, otherwise append
filtered = [e for e in data if e.get('name') != new_entry['name']]
filtered.append(new_entry)

# Sort by Macro F1 desc, then Accuracy desc
filtered.sort(key=lambda e: (e.get('macroF1', 0), e.get('accuracy', 0)), reverse=True)

# Re-rank
for idx, entry in enumerate(filtered, start=1):
    entry['rank'] = idx

leaderboard_path.write_text(json.dumps(filtered, indent=2))
PY

      - name: Show updated leaderboard snippet
        run: |
          head -n 40 web/src/app/leaderboard/data.json

      - name: Create leaderboard update PR
        uses: peter-evans/create-pull-request@v6
        with:
          commit-message: "chore: update leaderboard for ${{ inputs.team_name }}"
          title: "Update leaderboard: ${{ inputs.team_name }}"
          body: |
            ## Automated leaderboard update
            - Team: ${{ inputs.team_name }}
            - Macro F1: ${{ steps.metrics.outputs.macro_f1 }}
            - Accuracy: ${{ steps.metrics.outputs.accuracy }}
            - Weighted F1: ${{ steps.metrics.outputs.weighted_f1 }}
            - Params: ${{ inputs.params }}
            - Train time: ${{ inputs.train_time }}
            - Submission: ${{ inputs.submission_path }}

            Generated by `update-leaderboard.yml`.
          branch: leaderboard/update-${{ github.run_id }}
          delete-branch: true
          add-paths: |
            web/src/app/leaderboard/data.json
    
      - name: Upload evaluation artifact
        uses: actions/upload-artifact@v4
        with:
          name: leaderboard-evaluation
          path: |
            ${{ steps.metrics.outputs.results_path }}
            ${{ inputs.submission_path }}/predictions.csv
