name: Update Leaderboard

on:
  workflow_dispatch:
    inputs:
      submission_path:
        description: 'Path to submission directory (e.g., submissions/team-name)'
        required: true
      team_name:
        description: 'Display name for the leaderboard entry'
        required: true
      params:
        description: 'Model parameter count (e.g., 72K)'
        required: false
        default: 'N/A'
      train_time:
        description: 'Training time (e.g., ~3m 10s)'
        required: false
        default: 'N/A'
      is_baseline:
        description: 'Mark as baseline entry'
        required: false
        default: 'false'
      submitted_at:
        description: 'Optional submission label (date/PR/commit)'
        required: false
        default: ''

permissions:
  contents: write
  pull-requests: write

jobs:
  update:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python 3.10
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install numpy pandas scikit-learn torch
          pip install torch-geometric -f https://data.pyg.org/whl/torch-2.0.0+cpu.html

      - name: Prepare challenge data
        run: python scripts/prepare_data.py --seed 42

      - name: Evaluate submission on validation split
        run: |
          python scripts/evaluate.py \
            --predictions "${{ inputs.submission_path }}/predictions.csv" \
            --ground_truth val \
            --detailed

      - name: Extract metrics
        id: metrics
        run: |
          RESULTS_PATH="${{ inputs.submission_path }}/evaluation_results.json"
          if [ ! -f "$RESULTS_PATH" ]; then
            echo "results file not found: $RESULTS_PATH" >&2
            exit 1
          fi
          echo "results_path=$RESULTS_PATH" >> "$GITHUB_OUTPUT"
          echo "macro_f1=$(jq -r '.macro_f1' "$RESULTS_PATH")" >> "$GITHUB_OUTPUT"
          echo "accuracy=$(jq -r '.accuracy' "$RESULTS_PATH")" >> "$GITHUB_OUTPUT"
          echo "weighted_f1=$(jq -r '.weighted_f1' "$RESULTS_PATH")" >> "$GITHUB_OUTPUT"

      - name: Update leaderboard data
        run: |
          python scripts/update_leaderboard_ci.py \
            --submission "${{ inputs.submission_path }}" \
            --team-name "${{ inputs.team_name }}" \
            --params "${{ inputs.params }}" \
            --train-time "${{ inputs.train_time }}" \
            --is-baseline "${{ inputs.is_baseline }}" \
            --submitted-at "${{ inputs.submitted_at }}"

      - name: Show updated leaderboard snippet
        run: |
          head -n 40 web/src/app/leaderboard/data.json

      - name: Create leaderboard update PR
        uses: peter-evans/create-pull-request@v6
        with:
          commit-message: "chore: update leaderboard for ${{ inputs.team_name }}"
          title: "Update leaderboard: ${{ inputs.team_name }}"
          body: |
            ## Automated leaderboard update
            - Team: ${{ inputs.team_name }}
            - Macro F1: ${{ steps.metrics.outputs.macro_f1 }}
            - Accuracy: ${{ steps.metrics.outputs.accuracy }}
            - Weighted F1: ${{ steps.metrics.outputs.weighted_f1 }}
            - Params: ${{ inputs.params }}
            - Train time: ${{ inputs.train_time }}
            - Submission: ${{ inputs.submission_path }}

            Generated by `update-leaderboard.yml`.
          branch: leaderboard/update-${{ github.run_id }}
          delete-branch: true
          add-paths: |
            web/src/app/leaderboard/data.json
    
      - name: Upload evaluation artifact
        uses: actions/upload-artifact@v4
        with:
          name: leaderboard-evaluation
          path: |
            ${{ steps.metrics.outputs.results_path }}
            ${{ inputs.submission_path }}/predictions.csv
